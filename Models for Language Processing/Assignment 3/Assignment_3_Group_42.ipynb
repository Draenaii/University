{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3**\n",
        "\n",
        "In this assignment, we will follow up on the notion of interannotator agreement, and will do exercises on the topic of translation. \n",
        "\n",
        "**This is the first part of Assignment 3. Part 2 will be added later.**"
      ],
      "metadata": {
        "id": "02FNhhkQq6YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __3.1 Cross-linguistic variation__\n",
        "\n",
        "For starters, we will look at the typological properties of languages, as defined e.g. in WALS (https://wals.info/).\n",
        "\n",
        "Let's determine the word order parameters of Dutch.\n",
        "For statistics of word order counts in Dutch, use the first 20 (complete) instances of each construction type from the OPUS corpus:\n",
        "https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-nl/opus.en-nl-dev.nl\n"
      ],
      "metadata": {
        "id": "Rq426FTd1Hho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.1. What is the dominant order of Subject and Verb in Dutch? Is an alternative order possible? Confirm your answer by citing the frequency count among the first 20 instances of the relevant construction in the OPUS corpus. On each line, list an example preceded by a label (VS or SV). Each example should be a fragment of a sentence from OPUS with a subject noun or pronoun and a finite verb.\n",
        "\n",
        "**Answer:** The dominant order is SV. An alternative is possible: VS.\n",
        "1. VS 'was ze'\n",
        "2. SV 'ik weet'\n",
        "3. VS 'gaat dat'\n",
        "4. SV 'Wat hoort'\n",
        "5. SV 'Die pillen hadden'\n",
        "6. SV 'De uitvoerprijs werd'\n",
        "7. SV 'de gegevens die verstrekt werden'\n",
        "8. SV 'We zullen'\n",
        "9. SV 'het grote aantal weeskinderen ... wacht'\n",
        "10. SV 'het grote aantal weeskinderen dat op adoptie wacht ... zou'\n",
        "11. SV 'een belangrijke klap zijn'\n",
        "12. SV 'Ik zal'\n",
        "13. SV 'Nederland had'\n",
        "14. SV 'Ik weet'\n",
        "15. SV 'hij ... is'\n",
        "16. SV 'het is'\n",
        "17. VS 'kan deze informatie'\n",
        "18. SV 'Ik ben'\n",
        "19. SV 'Ik kan'\n",
        "20. VS 'Kan ik'\n",
        "\n"
      ],
      "metadata": {
        "id": "t57Ponlm1dXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.2. (1 point) What is the dominant order of Genitive and Noun in Dutch? Genitive is any constituent expressing a possessor, e.g. _**my** mother_, _**John's** mother_, _the mother **of his friend**_. Confirm your answer by citing the frequency count among the first 20 instances of the relevant construction in the OPUS corpus. Count possessive pronouns (e.g. _mijn_, _jouw_) that modify nouns as genitives. Also count any van+NP modifying a noun as a genitive. On each line, list an example preceded by a label (GenN or NGen). Each example should be a fragment of a sentence from OPUS with a noun and a possessive, e.g. _wiens_, _Johans_, or _van Nederland_.\n",
        "\n",
        "**Answer:** The dominant order is Noun-Genitive.\n",
        "1. GN 'mijn idee'\n",
        "2. NG 'de hoge autoriteiten van het vampirisme.'\n",
        "3. GN 'je kamer'\n",
        "4. GN 'zijn gedachten'\n",
        "5. NG 'het werk van onze collega Eurlings'\n",
        "6. GN 'wiens verslag'\n",
        "7. GN 'onze verdachte'\n",
        "8. NG 'de resoluties van de Veiligheidsraad'\n",
        "9. NG 'de Veiligheidsraad van de VN'\n",
        "10. NG 'aanzien van SyriÃ'\n",
        "11. GN 'je hart'\n",
        "12. GN 'jouw werk'\n",
        "13. NG 'de oer-oorzaak van alle oorzaken'\n",
        "14. NG 'strategisch belangrijke wijzigingen van zogenaamde specificaties'\n",
        "15. NG 'zogenaamde specificaties van de projecten'\n",
        "16. NG 'die van de autoindustrie'\n",
        "17. NG 'sectoren van de toekomst'\n",
        "18. GN 'mijn verslag'\n",
        "19. NG 'een verslag over het milieu'\n",
        "20. NG 'controle van de gemiddelde uitlaatemissies na een koude start'\n",
        "\n",
        "\n",
        "12X Noun-Genetive, niet heel overtuigend maar meer dan het omgekeerde"
      ],
      "metadata": {
        "id": "VETx1idh17Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.3. (1 point) What is the dominant order of Adposition and Noun Phrase in Dutch? Confirm your answer by citing the frequency count among the first 20 instances of the relevant construction in the OPUS corpus.On each line, list an example preceded by a label (AdpN or NAdp). Each example should be a fragment of a sentence from OPUS with a noun and a possessive, e.g. \n",
        "\n",
        "AdpN _Naar mijn idee_.\n",
        "\n",
        "Does the other order exist in Dutch? If yes, give an example.\n",
        "\n",
        "\n",
        "**Answer:** It does, for example: \"Jaap liep het veld op\".\n",
        "1. AdpN 'naar mijn idee'\n",
        "2. AdpN 'door de Mitsui-groep en Eurostat'\n",
        "3. AdpN 'met de hoge autoriteiten'\n",
        "4. AdpN 'Met het oog'\n",
        "5. AdpN 'op het grote aantal weeskinderen'\n",
        "6. AdpN 'op adoptie wacht'\n",
        "7. AdpN 'op de adoptie'\n",
        "8. AdpN 'van niet-Europese kinderen'\n",
        "9. AdpN 'voor de kinderhandel'\n",
        "10. AdpN 'van een Europees register'\n",
        "11. AdpN 'van zedendelinquenten'\n",
        "12. NAdp 'zwaardere straffen voor'\n",
        "13. NAdp 'sexueel contact met'\n",
        "14. AdpN 'van kinderporno'\n",
        "15. AdpN 'aan het werk'\n",
        "16. AdpN 'van onze collega Eurlings'\n",
        "17. AdpN 'van de huidige situatie'\n",
        "18. AdpN 'om de invoering'\n",
        "19. AdpN 'van douanerechten en heffingen'\n",
        "20. AdpN 'van het gemeenschappelijk landbouwbeleid'  "
      ],
      "metadata": {
        "id": "ySenTxyZ3iRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3.1.4. You can confirm the word order parameters you determined for Dutch using WALS. \n",
        "\n",
        "(1 point) Now, with the help of WALS, find a language X spoken outside of South America that is opposite to Dutch with respect to parameters above: the dominant order of Subject and the Verb, the dominant order of adposition and Noun Phrase, and the dominant order of genitive and noun. Note that WALS uses many more options than just \"Prepositions\" and \"Postpositions\". Ignore these further options.\n",
        "What is the name of the language and in what country is it spoken?\n",
        "\n",
        "**Answer:** It is Tepehuan (Northern) and is spoken in Mexico.\n"
      ],
      "metadata": {
        "id": "Gl2EuCjJ4B0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.5 (1 point) Look up more information in WALS about the language X from the previous question. How are definite and indefinite articles expressed in X? Does language X have gender?\n",
        "\n",
        "**Answer:** No indefinite, but definite article. No gender distinctions."
      ],
      "metadata": {
        "id": "lhA-PPW-4ZQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.6. How would the translation of the phrase \"with rice\" look in language X? Since you don’t know any words or morphemes in X, use a word by word translation.\n",
        "\n",
        "**Answer:** rice with\n"
      ],
      "metadata": {
        "id": "cIoCDHvh4nr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.7. (1 point) Give an example of a sentence in X that cannot be unambiguously translated into English. Since you don't know any words or morphemes in X, use a word by word translation.\n",
        "\n",
        "**Answer:** I am hers, and she is mine."
      ],
      "metadata": {
        "id": "CeyREDS14xIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.8. (2 points) How would the translation of the sentence _The child ate an avocado_ look in language X? Assume the most likely (=dominant) word order in your example. Since you don’t know any words or morphemes in X, use a word by word translation in your answer. Be sure to use articles in X where necessary.\n",
        "\n",
        "**Answer:** Ate the child the avocado"
      ],
      "metadata": {
        "id": "WPrr7cxA5cNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __3.2 Assessing multi-rater annotation__\n",
        "\n",
        "Let's start by opening the annotation data we created last week. The following code imports data from the relevant Google spreadsheet:"
      ],
      "metadata": {
        "id": "X4oZezr7VNBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqZd_jABczKz"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "gsheets = gc.open_by_url('https://docs.google.com/spreadsheets/d/1-FZHXPaHwFFcytMXcaXDBT8XyKTCy4wpP7DRT4s2AEo/edit?usp=sharing')\n",
        "sheet = gsheets.worksheet('Form Responses 1').get_all_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.1. (1 point) Filter the data we imported, creating a list of entries called *entries* where each element corresponds to responses from a single annotator and is a list of their chosen labels (don't include the timestamp). For example, the list of entries will include as one of the entries the list of judgments ['A entails B',\n",
        " 'Neutral',\n",
        " 'Neutral',...]. Make sure to omit the annotators who skipped some of the sentence pairs. How many annotators are we left with?"
      ],
      "metadata": {
        "id": "p0QE2DABtIwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR ANSWER HERE\n",
        "entries = []\n",
        "for line in sheet:\n",
        "  if '' in line or line == sheet[0]:\n",
        "    continue\n",
        "  \n",
        "  entries.append(line[1:])\n",
        "#END OF YOUR CODE\n",
        "\n",
        "print(\"Number of annotations:\",len(entries))"
      ],
      "metadata": {
        "id": "b407kTWMVR4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3ae961-8870-4455-dba9-b907a8bc2b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of annotations: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.2. (2 points) Agreement in annotation can be quantified in different ways for different purposes.\n",
        "\n",
        "The authors of the original SICK dataset quantified inter-rater agreement for the entailment annotation as agreement with the majority:\n",
        "\n",
        "*Inter–rater\n",
        "agreement for the entailment task was .84, computed as the\n",
        "average proportion of the majority vote across pairs and\n",
        "indicating that, as an average, 84% of participants agreed\n",
        "with the majority vote in each pair.* (Marelli et al. 2014, p.221).\n",
        "\n",
        "How does our annotation compare with the orignal SICK? Presumably it could be much less consistent because the annotators didn't receive specific instructions.\n",
        "\n",
        "Compute the list of majority labels *aggregated* that will have the form ['A entails B', 'A entails B', etc.] and the agreement with majority as quantified in the SICK paper."
      ],
      "metadata": {
        "id": "kDAM63w5Vi2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR ANSWER HERE\n",
        "majority_votes = []\n",
        "aggregated = []\n",
        "\n",
        "for pair_index in range(len(entries[0])):\n",
        "    \n",
        "    vote_counts = {}\n",
        "    for entry in entries:\n",
        "        vote = entry[pair_index]\n",
        "        if vote in vote_counts:\n",
        "            vote_counts[vote] += 1\n",
        "        else:\n",
        "            vote_counts[vote] = 1\n",
        "    \n",
        "    maj = max(vote_counts, key=vote_counts.get)\n",
        "    maj_prop = vote_counts[maj] / len(entries)\n",
        "    \n",
        "    aggregated.append(maj)\n",
        "    majority_votes.append(maj_prop)\n",
        "\n",
        "agreement = sum(majority_votes) / len(entries[0])\n",
        "  \n",
        "\n",
        "#END OF YOUR CODE\n",
        "print(aggregated)\n",
        "print(agreement)"
      ],
      "metadata": {
        "id": "jwYF_WdpVkAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa79f6e-c9d5-406b-c6c4-111cb452001a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A entails B', 'A entails B', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'A entails B', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'A contradicts B', 'Neutral', 'A entails B', 'A contradicts B', 'A contradicts B', 'A contradicts B']\n",
            "0.8678571428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use NLTK to calculate agreement between annotators with Cohen's kappa and Fleiss's kappa\n",
        "https://www.nltk.org/api/nltk.metrics.agreement.html\n",
        "\n",
        "3.2.3. (1 point) Start by preparing the data for feeding into NLTK in the form of a list of triples (see documentation for details). How big is our data?"
      ],
      "metadata": {
        "id": "0aeTDF14V0RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR ANSWER HERE\n",
        "data_array = []\n",
        "for k, coder in enumerate(entries):\n",
        "  i = 0\n",
        "  for label in coder:\n",
        "    data_array.append((f'c{k}', i, label))\n",
        "    i += 1\n",
        "\n",
        "#END OF YOUR CODE\n",
        "print(len(data_array))\n",
        "print(data_array)"
      ],
      "metadata": {
        "id": "l9JOE7HLV38t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86620fd-603d-4f6f-bec7-c94c3adb5a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "560\n",
            "[('c0', 0, 'A entails B'), ('c0', 1, 'Neutral'), ('c0', 2, 'Neutral'), ('c0', 3, 'Neutral'), ('c0', 4, 'Neutral'), ('c0', 5, 'Neutral'), ('c0', 6, 'Neutral'), ('c0', 7, 'A entails B'), ('c0', 8, 'Neutral'), ('c0', 9, 'Neutral'), ('c0', 10, 'Neutral'), ('c0', 11, 'Neutral'), ('c0', 12, 'Neutral'), ('c0', 13, 'Neutral'), ('c0', 14, 'A contradicts B'), ('c0', 15, 'Neutral'), ('c0', 16, 'A entails B'), ('c0', 17, 'A contradicts B'), ('c0', 18, 'A contradicts B'), ('c0', 19, 'A contradicts B'), ('c1', 0, 'A entails B'), ('c1', 1, 'Neutral'), ('c1', 2, 'Neutral'), ('c1', 3, 'Neutral'), ('c1', 4, 'Neutral'), ('c1', 5, 'Neutral'), ('c1', 6, 'Neutral'), ('c1', 7, 'A entails B'), ('c1', 8, 'Neutral'), ('c1', 9, 'Neutral'), ('c1', 10, 'Neutral'), ('c1', 11, 'Neutral'), ('c1', 12, 'Neutral'), ('c1', 13, 'Neutral'), ('c1', 14, 'Neutral'), ('c1', 15, 'Neutral'), ('c1', 16, 'A entails B'), ('c1', 17, 'A contradicts B'), ('c1', 18, 'A contradicts B'), ('c1', 19, 'A contradicts B'), ('c2', 0, 'A entails B'), ('c2', 1, 'A entails B'), ('c2', 2, 'Neutral'), ('c2', 3, 'Neutral'), ('c2', 4, 'Neutral'), ('c2', 5, 'Neutral'), ('c2', 6, 'Neutral'), ('c2', 7, 'A entails B'), ('c2', 8, 'Neutral'), ('c2', 9, 'Neutral'), ('c2', 10, 'Neutral'), ('c2', 11, 'Neutral'), ('c2', 12, 'Neutral'), ('c2', 13, 'Neutral'), ('c2', 14, 'A contradicts B'), ('c2', 15, 'Neutral'), ('c2', 16, 'A entails B'), ('c2', 17, 'A contradicts B'), ('c2', 18, 'A contradicts B'), ('c2', 19, 'A contradicts B'), ('c3', 0, 'A entails B'), ('c3', 1, 'A entails B'), ('c3', 2, 'Neutral'), ('c3', 3, 'Neutral'), ('c3', 4, 'Neutral'), ('c3', 5, 'Neutral'), ('c3', 6, 'Neutral'), ('c3', 7, 'A entails B'), ('c3', 8, 'Neutral'), ('c3', 9, 'Neutral'), ('c3', 10, 'Neutral'), ('c3', 11, 'Neutral'), ('c3', 12, 'Neutral'), ('c3', 13, 'Neutral'), ('c3', 14, 'Neutral'), ('c3', 15, 'Neutral'), ('c3', 16, 'A entails B'), ('c3', 17, 'A contradicts B'), ('c3', 18, 'A contradicts B'), ('c3', 19, 'A contradicts B'), ('c4', 0, 'A entails B'), ('c4', 1, 'Neutral'), ('c4', 2, 'Neutral'), ('c4', 3, 'Neutral'), ('c4', 4, 'Neutral'), ('c4', 5, 'A entails B'), ('c4', 6, 'Neutral'), ('c4', 7, 'A entails B'), ('c4', 8, 'Neutral'), ('c4', 9, 'A entails B'), ('c4', 10, 'A entails B'), ('c4', 11, 'Neutral'), ('c4', 12, 'Neutral'), ('c4', 13, 'Neutral'), ('c4', 14, 'Neutral'), ('c4', 15, 'A contradicts B'), ('c4', 16, 'A entails B'), ('c4', 17, 'A contradicts B'), ('c4', 18, 'Neutral'), ('c4', 19, 'A contradicts B'), ('c5', 0, 'A entails B'), ('c5', 1, 'Neutral'), ('c5', 2, 'Neutral'), ('c5', 3, 'Neutral'), ('c5', 4, 'Neutral'), ('c5', 5, 'Neutral'), ('c5', 6, 'Neutral'), ('c5', 7, 'A entails B'), ('c5', 8, 'Neutral'), ('c5', 9, 'Neutral'), ('c5', 10, 'Neutral'), ('c5', 11, 'Neutral'), ('c5', 12, 'Neutral'), ('c5', 13, 'Neutral'), ('c5', 14, 'Neutral'), ('c5', 15, 'Neutral'), ('c5', 16, 'A entails B'), ('c5', 17, 'A contradicts B'), ('c5', 18, 'A contradicts B'), ('c5', 19, 'A contradicts B'), ('c6', 0, 'A entails B'), ('c6', 1, 'A entails B'), ('c6', 2, 'A entails B'), ('c6', 3, 'A contradicts B'), ('c6', 4, 'A contradicts B'), ('c6', 5, 'A entails B'), ('c6', 6, 'A contradicts B'), ('c6', 7, 'A contradicts B'), ('c6', 8, 'A contradicts B'), ('c6', 9, 'A contradicts B'), ('c6', 10, 'A entails B'), ('c6', 11, 'Neutral'), ('c6', 12, 'Neutral'), ('c6', 13, 'Neutral'), ('c6', 14, 'A contradicts B'), ('c6', 15, 'A contradicts B'), ('c6', 16, 'A entails B'), ('c6', 17, 'A contradicts B'), ('c6', 18, 'A contradicts B'), ('c6', 19, 'A contradicts B'), ('c7', 0, 'A entails B'), ('c7', 1, 'A entails B'), ('c7', 2, 'Neutral'), ('c7', 3, 'Neutral'), ('c7', 4, 'Neutral'), ('c7', 5, 'Neutral'), ('c7', 6, 'Neutral'), ('c7', 7, 'A entails B'), ('c7', 8, 'Neutral'), ('c7', 9, 'Neutral'), ('c7', 10, 'Neutral'), ('c7', 11, 'Neutral'), ('c7', 12, 'Neutral'), ('c7', 13, 'Neutral'), ('c7', 14, 'A contradicts B'), ('c7', 15, 'Neutral'), ('c7', 16, 'A entails B'), ('c7', 17, 'A contradicts B'), ('c7', 18, 'A contradicts B'), ('c7', 19, 'A contradicts B'), ('c8', 0, 'A entails B'), ('c8', 1, 'Neutral'), ('c8', 2, 'Neutral'), ('c8', 3, 'A contradicts B'), ('c8', 4, 'Neutral'), ('c8', 5, 'Neutral'), ('c8', 6, 'Neutral'), ('c8', 7, 'A entails B'), ('c8', 8, 'Neutral'), ('c8', 9, 'Neutral'), ('c8', 10, 'Neutral'), ('c8', 11, 'A contradicts B'), ('c8', 12, 'Neutral'), ('c8', 13, 'Neutral'), ('c8', 14, 'A contradicts B'), ('c8', 15, 'A contradicts B'), ('c8', 16, 'A entails B'), ('c8', 17, 'A contradicts B'), ('c8', 18, 'A contradicts B'), ('c8', 19, 'A contradicts B'), ('c9', 0, 'A entails B'), ('c9', 1, 'Neutral'), ('c9', 2, 'A entails B'), ('c9', 3, 'A contradicts B'), ('c9', 4, 'Neutral'), ('c9', 5, 'Neutral'), ('c9', 6, 'A contradicts B'), ('c9', 7, 'A entails B'), ('c9', 8, 'Neutral'), ('c9', 9, 'A contradicts B'), ('c9', 10, 'Neutral'), ('c9', 11, 'Neutral'), ('c9', 12, 'Neutral'), ('c9', 13, 'Neutral'), ('c9', 14, 'A contradicts B'), ('c9', 15, 'A contradicts B'), ('c9', 16, 'A entails B'), ('c9', 17, 'A contradicts B'), ('c9', 18, 'A contradicts B'), ('c9', 19, 'A contradicts B'), ('c10', 0, 'Neutral'), ('c10', 1, 'Neutral'), ('c10', 2, 'Neutral'), ('c10', 3, 'Neutral'), ('c10', 4, 'A entails B'), ('c10', 5, 'A entails B'), ('c10', 6, 'Neutral'), ('c10', 7, 'A entails B'), ('c10', 8, 'Neutral'), ('c10', 9, 'Neutral'), ('c10', 10, 'Neutral'), ('c10', 11, 'Neutral'), ('c10', 12, 'Neutral'), ('c10', 13, 'Neutral'), ('c10', 14, 'A contradicts B'), ('c10', 15, 'Neutral'), ('c10', 16, 'A entails B'), ('c10', 17, 'A contradicts B'), ('c10', 18, 'A contradicts B'), ('c10', 19, 'A contradicts B'), ('c11', 0, 'A entails B'), ('c11', 1, 'A entails B'), ('c11', 2, 'Neutral'), ('c11', 3, 'Neutral'), ('c11', 4, 'A entails B'), ('c11', 5, 'Neutral'), ('c11', 6, 'A contradicts B'), ('c11', 7, 'A entails B'), ('c11', 8, 'Neutral'), ('c11', 9, 'Neutral'), ('c11', 10, 'Neutral'), ('c11', 11, 'Neutral'), ('c11', 12, 'Neutral'), ('c11', 13, 'Neutral'), ('c11', 14, 'A contradicts B'), ('c11', 15, 'A contradicts B'), ('c11', 16, 'A entails B'), ('c11', 17, 'A contradicts B'), ('c11', 18, 'A contradicts B'), ('c11', 19, 'A contradicts B'), ('c12', 0, 'A entails B'), ('c12', 1, 'A entails B'), ('c12', 2, 'Neutral'), ('c12', 3, 'Neutral'), ('c12', 4, 'Neutral'), ('c12', 5, 'Neutral'), ('c12', 6, 'Neutral'), ('c12', 7, 'A entails B'), ('c12', 8, 'Neutral'), ('c12', 9, 'Neutral'), ('c12', 10, 'Neutral'), ('c12', 11, 'Neutral'), ('c12', 12, 'Neutral'), ('c12', 13, 'Neutral'), ('c12', 14, 'A contradicts B'), ('c12', 15, 'A contradicts B'), ('c12', 16, 'A entails B'), ('c12', 17, 'A contradicts B'), ('c12', 18, 'A contradicts B'), ('c12', 19, 'A contradicts B'), ('c13', 0, 'Neutral'), ('c13', 1, 'A entails B'), ('c13', 2, 'Neutral'), ('c13', 3, 'Neutral'), ('c13', 4, 'Neutral'), ('c13', 5, 'Neutral'), ('c13', 6, 'Neutral'), ('c13', 7, 'A entails B'), ('c13', 8, 'Neutral'), ('c13', 9, 'Neutral'), ('c13', 10, 'Neutral'), ('c13', 11, 'Neutral'), ('c13', 12, 'Neutral'), ('c13', 13, 'Neutral'), ('c13', 14, 'A contradicts B'), ('c13', 15, 'A contradicts B'), ('c13', 16, 'A entails B'), ('c13', 17, 'A contradicts B'), ('c13', 18, 'A contradicts B'), ('c13', 19, 'A contradicts B'), ('c14', 0, 'A entails B'), ('c14', 1, 'A entails B'), ('c14', 2, 'Neutral'), ('c14', 3, 'Neutral'), ('c14', 4, 'Neutral'), ('c14', 5, 'Neutral'), ('c14', 6, 'Neutral'), ('c14', 7, 'A entails B'), ('c14', 8, 'A contradicts B'), ('c14', 9, 'Neutral'), ('c14', 10, 'Neutral'), ('c14', 11, 'Neutral'), ('c14', 12, 'Neutral'), ('c14', 13, 'Neutral'), ('c14', 14, 'A contradicts B'), ('c14', 15, 'Neutral'), ('c14', 16, 'A entails B'), ('c14', 17, 'A contradicts B'), ('c14', 18, 'A contradicts B'), ('c14', 19, 'A contradicts B'), ('c15', 0, 'A entails B'), ('c15', 1, 'A entails B'), ('c15', 2, 'Neutral'), ('c15', 3, 'Neutral'), ('c15', 4, 'Neutral'), ('c15', 5, 'Neutral'), ('c15', 6, 'Neutral'), ('c15', 7, 'A entails B'), ('c15', 8, 'A entails B'), ('c15', 9, 'Neutral'), ('c15', 10, 'Neutral'), ('c15', 11, 'Neutral'), ('c15', 12, 'Neutral'), ('c15', 13, 'Neutral'), ('c15', 14, 'Neutral'), ('c15', 15, 'A contradicts B'), ('c15', 16, 'A entails B'), ('c15', 17, 'A contradicts B'), ('c15', 18, 'A contradicts B'), ('c15', 19, 'A contradicts B'), ('c16', 0, 'A entails B'), ('c16', 1, 'A entails B'), ('c16', 2, 'Neutral'), ('c16', 3, 'Neutral'), ('c16', 4, 'Neutral'), ('c16', 5, 'Neutral'), ('c16', 6, 'Neutral'), ('c16', 7, 'A entails B'), ('c16', 8, 'Neutral'), ('c16', 9, 'Neutral'), ('c16', 10, 'Neutral'), ('c16', 11, 'Neutral'), ('c16', 12, 'Neutral'), ('c16', 13, 'Neutral'), ('c16', 14, 'A contradicts B'), ('c16', 15, 'Neutral'), ('c16', 16, 'A entails B'), ('c16', 17, 'A contradicts B'), ('c16', 18, 'A contradicts B'), ('c16', 19, 'A contradicts B'), ('c17', 0, 'A entails B'), ('c17', 1, 'A entails B'), ('c17', 2, 'Neutral'), ('c17', 3, 'Neutral'), ('c17', 4, 'Neutral'), ('c17', 5, 'Neutral'), ('c17', 6, 'Neutral'), ('c17', 7, 'A entails B'), ('c17', 8, 'Neutral'), ('c17', 9, 'Neutral'), ('c17', 10, 'Neutral'), ('c17', 11, 'Neutral'), ('c17', 12, 'Neutral'), ('c17', 13, 'Neutral'), ('c17', 14, 'A contradicts B'), ('c17', 15, 'Neutral'), ('c17', 16, 'A entails B'), ('c17', 17, 'A contradicts B'), ('c17', 18, 'A contradicts B'), ('c17', 19, 'A contradicts B'), ('c18', 0, 'A entails B'), ('c18', 1, 'A entails B'), ('c18', 2, 'Neutral'), ('c18', 3, 'Neutral'), ('c18', 4, 'Neutral'), ('c18', 5, 'Neutral'), ('c18', 6, 'Neutral'), ('c18', 7, 'A entails B'), ('c18', 8, 'Neutral'), ('c18', 9, 'Neutral'), ('c18', 10, 'Neutral'), ('c18', 11, 'Neutral'), ('c18', 12, 'Neutral'), ('c18', 13, 'Neutral'), ('c18', 14, 'A contradicts B'), ('c18', 15, 'Neutral'), ('c18', 16, 'Neutral'), ('c18', 17, 'A contradicts B'), ('c18', 18, 'A contradicts B'), ('c18', 19, 'A contradicts B'), ('c19', 0, 'A entails B'), ('c19', 1, 'A entails B'), ('c19', 2, 'Neutral'), ('c19', 3, 'Neutral'), ('c19', 4, 'Neutral'), ('c19', 5, 'A entails B'), ('c19', 6, 'A contradicts B'), ('c19', 7, 'A entails B'), ('c19', 8, 'Neutral'), ('c19', 9, 'A contradicts B'), ('c19', 10, 'Neutral'), ('c19', 11, 'Neutral'), ('c19', 12, 'Neutral'), ('c19', 13, 'Neutral'), ('c19', 14, 'A contradicts B'), ('c19', 15, 'A contradicts B'), ('c19', 16, 'A entails B'), ('c19', 17, 'A contradicts B'), ('c19', 18, 'A contradicts B'), ('c19', 19, 'A contradicts B'), ('c20', 0, 'A entails B'), ('c20', 1, 'A entails B'), ('c20', 2, 'Neutral'), ('c20', 3, 'Neutral'), ('c20', 4, 'Neutral'), ('c20', 5, 'Neutral'), ('c20', 6, 'Neutral'), ('c20', 7, 'A entails B'), ('c20', 8, 'Neutral'), ('c20', 9, 'Neutral'), ('c20', 10, 'Neutral'), ('c20', 11, 'Neutral'), ('c20', 12, 'Neutral'), ('c20', 13, 'Neutral'), ('c20', 14, 'A contradicts B'), ('c20', 15, 'A contradicts B'), ('c20', 16, 'A entails B'), ('c20', 17, 'A contradicts B'), ('c20', 18, 'A contradicts B'), ('c20', 19, 'A contradicts B'), ('c21', 0, 'A entails B'), ('c21', 1, 'A entails B'), ('c21', 2, 'Neutral'), ('c21', 3, 'A contradicts B'), ('c21', 4, 'A entails B'), ('c21', 5, 'Neutral'), ('c21', 6, 'Neutral'), ('c21', 7, 'A entails B'), ('c21', 8, 'A contradicts B'), ('c21', 9, 'A entails B'), ('c21', 10, 'Neutral'), ('c21', 11, 'Neutral'), ('c21', 12, 'Neutral'), ('c21', 13, 'Neutral'), ('c21', 14, 'A contradicts B'), ('c21', 15, 'Neutral'), ('c21', 16, 'A entails B'), ('c21', 17, 'A contradicts B'), ('c21', 18, 'A contradicts B'), ('c21', 19, 'A contradicts B'), ('c22', 0, 'A entails B'), ('c22', 1, 'Neutral'), ('c22', 2, 'Neutral'), ('c22', 3, 'Neutral'), ('c22', 4, 'Neutral'), ('c22', 5, 'Neutral'), ('c22', 6, 'Neutral'), ('c22', 7, 'A entails B'), ('c22', 8, 'Neutral'), ('c22', 9, 'Neutral'), ('c22', 10, 'Neutral'), ('c22', 11, 'Neutral'), ('c22', 12, 'Neutral'), ('c22', 13, 'Neutral'), ('c22', 14, 'A contradicts B'), ('c22', 15, 'Neutral'), ('c22', 16, 'A entails B'), ('c22', 17, 'A contradicts B'), ('c22', 18, 'A contradicts B'), ('c22', 19, 'A contradicts B'), ('c23', 0, 'A entails B'), ('c23', 1, 'A entails B'), ('c23', 2, 'Neutral'), ('c23', 3, 'A contradicts B'), ('c23', 4, 'A entails B'), ('c23', 5, 'Neutral'), ('c23', 6, 'Neutral'), ('c23', 7, 'A entails B'), ('c23', 8, 'Neutral'), ('c23', 9, 'A entails B'), ('c23', 10, 'Neutral'), ('c23', 11, 'A contradicts B'), ('c23', 12, 'A entails B'), ('c23', 13, 'Neutral'), ('c23', 14, 'A contradicts B'), ('c23', 15, 'Neutral'), ('c23', 16, 'A entails B'), ('c23', 17, 'A contradicts B'), ('c23', 18, 'A contradicts B'), ('c23', 19, 'A contradicts B'), ('c24', 0, 'A entails B'), ('c24', 1, 'Neutral'), ('c24', 2, 'Neutral'), ('c24', 3, 'Neutral'), ('c24', 4, 'Neutral'), ('c24', 5, 'Neutral'), ('c24', 6, 'Neutral'), ('c24', 7, 'Neutral'), ('c24', 8, 'Neutral'), ('c24', 9, 'Neutral'), ('c24', 10, 'Neutral'), ('c24', 11, 'Neutral'), ('c24', 12, 'Neutral'), ('c24', 13, 'Neutral'), ('c24', 14, 'Neutral'), ('c24', 15, 'Neutral'), ('c24', 16, 'A entails B'), ('c24', 17, 'A contradicts B'), ('c24', 18, 'A contradicts B'), ('c24', 19, 'Neutral'), ('c25', 0, 'Neutral'), ('c25', 1, 'Neutral'), ('c25', 2, 'Neutral'), ('c25', 3, 'Neutral'), ('c25', 4, 'Neutral'), ('c25', 5, 'Neutral'), ('c25', 6, 'Neutral'), ('c25', 7, 'A entails B'), ('c25', 8, 'Neutral'), ('c25', 9, 'Neutral'), ('c25', 10, 'Neutral'), ('c25', 11, 'Neutral'), ('c25', 12, 'Neutral'), ('c25', 13, 'Neutral'), ('c25', 14, 'Neutral'), ('c25', 15, 'Neutral'), ('c25', 16, 'A entails B'), ('c25', 17, 'A contradicts B'), ('c25', 18, 'A contradicts B'), ('c25', 19, 'A contradicts B'), ('c26', 0, 'A entails B'), ('c26', 1, 'A entails B'), ('c26', 2, 'Neutral'), ('c26', 3, 'Neutral'), ('c26', 4, 'Neutral'), ('c26', 5, 'Neutral'), ('c26', 6, 'A contradicts B'), ('c26', 7, 'A entails B'), ('c26', 8, 'Neutral'), ('c26', 9, 'Neutral'), ('c26', 10, 'Neutral'), ('c26', 11, 'Neutral'), ('c26', 12, 'Neutral'), ('c26', 13, 'Neutral'), ('c26', 14, 'A contradicts B'), ('c26', 15, 'A entails B'), ('c26', 16, 'A entails B'), ('c26', 17, 'A entails B'), ('c26', 18, 'A contradicts B'), ('c26', 19, 'A contradicts B'), ('c27', 0, 'A entails B'), ('c27', 1, 'A entails B'), ('c27', 2, 'Neutral'), ('c27', 3, 'Neutral'), ('c27', 4, 'Neutral'), ('c27', 5, 'Neutral'), ('c27', 6, 'Neutral'), ('c27', 7, 'A entails B'), ('c27', 8, 'A entails B'), ('c27', 9, 'Neutral'), ('c27', 10, 'Neutral'), ('c27', 11, 'Neutral'), ('c27', 12, 'Neutral'), ('c27', 13, 'Neutral'), ('c27', 14, 'A contradicts B'), ('c27', 15, 'Neutral'), ('c27', 16, 'A entails B'), ('c27', 17, 'A contradicts B'), ('c27', 18, 'A contradicts B'), ('c27', 19, 'A contradicts B')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.4. (1 point) Agreement between a pair of annotators\n",
        "\n",
        "*   What is the raw agrement between annotators 1 and 2?\n",
        "*   What is the expected chance agrement between annotators 1 and 2?\n",
        "*   What is Cohen's kappa for annotators 1 and 2, which quantifies the amount of agreement above chance? Note that the annotators are numbered from 0.\n",
        "\n",
        "Hint: if in doubt about which functions to use, click on \"[source]\" in the documentation page and read the source code, and/or check the returned values according to the formula for Cohen's kappa."
      ],
      "metadata": {
        "id": "3_e4LD_2WAsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import agreement\n",
        "annotation=agreement.AnnotationTask(data=data_array)\n",
        "\n",
        "#YOUR ANSWER HERE\n",
        "agr12 = annotation.Ao('c0', 'c1')\n",
        "exp12 = annotation.kappa_pairwise('c0', 'c1')\n",
        "kappa_12 = annotation.weighted_kappa_pairwise('c0','c1')\n",
        "\n",
        "#END OF YOUR CODE\n",
        "\n",
        "print(\"Raw observed agreement:\",agr12)\n",
        "print(\"Expected chance agreement:\",exp12)\n",
        "print(\"Cohen's kappa:\",kappa_12)"
      ],
      "metadata": {
        "id": "AQvE_40oWB2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f927043-43f7-4688-b42c-ba8e5e66879a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw observed agreement: 0.95\n",
            "Expected chance agreement: 0.898477157360406\n",
            "Cohen's kappa: 0.8984771573604061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.5. (1 point) What are the corresponding measures of agreement across all annotators? Report \n",
        "* the average agreement between pairs of annotators (the analog of raw agreement), as well as \n",
        "* three different ways of quantifying agreement above chance: \n",
        "  * average Cohen's kappa between different annotator pairs, \n",
        "  * (Davies and) Fleiss's kappa, and \n",
        "  * Krippendorff's alpha. "
      ],
      "metadata": {
        "id": "9x1ABg3DWO9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR ANSWER HERE\n",
        "annotation=agreement.AnnotationTask(data=data_array)\n",
        "avg_agr = annotation.avg_Ao()\n",
        "avgkappa = annotation.kappa()\n",
        "fleiss = annotation.multi_kappa()\n",
        "alpha = annotation.alpha()\n",
        "#END OF YOUR CODE\n",
        "\n",
        "print(\"Average unweighted agreement:\",avg_agr)\n",
        "print(\"Average Cohen's kappa:\",avgkappa)\n",
        "print(\"Fleiss kappa:\",fleiss)\n",
        "print(\"Krippendorff's alpha:\",alpha)"
      ],
      "metadata": {
        "id": "kf5OL30-WQG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e8442c-8564-426b-abc6-8ad6a5665f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average unweighted agreement: 0.7817460317460309\n",
            "Average Cohen's kappa: 0.6428170482914107\n",
            "Fleiss kappa: 0.6314619791608486\n",
            "Krippendorff's alpha: 0.6314397644666936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3 Realistic Data and Byte Pair Encoding**"
      ],
      "metadata": {
        "id": "g-_PYPlaPt9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural languages have open vocabularies with a long tail, i.e. there is a large number of very infrequent words. So in practice, it is not efficient (and often not realistic) to use every word as a processing unit whose properties have to be learned separately. Instead, less frequent words are broken into multiple subwords, usually via the byte pair encoding (BPE) algorithm. Now you will implement the BPE algorithm as it is introduced in SLP3, Chapter 2, p.19."
      ],
      "metadata": {
        "id": "JL7owChBA2yK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.1 (2 points) Define function ```bigrams``` that returns a list of bigrams in a list. Do NOT insert underscore (\"_\") within ```bigrams``` function."
      ],
      "metadata": {
        "id": "BqgTRlprbnv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bigrams(lst):\n",
        "  #YOUR CODE HERE\n",
        " # return [(lst[i], lst[i+1]) for i in range(len(lst) - 1) if lst[i] != '_' and lst[i+1] != '_']\n",
        "  return [(lst[i], lst[i+1]) for i in range(len(lst) - 1) if '_' not in lst[i] ]"
      ],
      "metadata": {
        "id": "25BQnEull47B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check what your function does to the list of characters in the word _tree_:"
      ],
      "metadata": {
        "id": "d3_EDFilupHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams(['t','r','e','e'])"
      ],
      "metadata": {
        "id": "H6itKxMnubFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b45cd20-dcd7-4977-c5cf-e9635b16517d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t', 'r'), ('r', 'e'), ('e', 'e')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.2 (2 points) Define function replace_bp that replaces each occurrence of the bigram nextbp with its concatenation. For example,\n",
        "\n",
        "```replace_bp(['t', 'r', 'e', 'e', '_'],'ee')```\n",
        "\n",
        "should give \n",
        "\n",
        "```['t','r', 'ee', '_']```"
      ],
      "metadata": {
        "id": "hKtOD1Nlb83w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_bp(y,nextbp):\n",
        "  #YOUR CODE HERE\n",
        "  replaced_lst = []\n",
        "  i = 0\n",
        "  while i < len(y):   #loop through the list\n",
        "    if i < len(y) - 1 and y[i] + y[i+1] == nextbp:    #if two consecutive items from the list make up the nextbp append the nextbp\n",
        "      replaced_lst.append(nextbp) \n",
        "      i += 2    #skip the next i since this has been added already\n",
        "    else:\n",
        "      replaced_lst.append(y[i])  #append the single item incase and move to the next item\n",
        "      i += 1\n",
        "  return replaced_lst\n",
        "\n",
        "y=['t','r','e','e','_']\n",
        "replace_bp(y,'ee')"
      ],
      "metadata": {
        "id": "86SaEQvWqzkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce276222-c086-470a-c272-21025611ea32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t', 'r', 'ee', '_']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.3 (8 points) Train BPE vocabulary for subword tokenization \n",
        "\n",
        "Now you can define a function bpe_train using replace_bp that reads a training corpus from file and returns a vocabulary of a certain size ```targetbpe``` as a list of strings. When reading the corpus from file within the definition of _bpe_train_ function, make sure to insert an underscore '\\_' symbol after each word so the original word breaks aren't lost. You can use the standard ```split()``` function to separate words from each other initially.\n",
        "\n",
        "You can use a Counter object to count symbols or pairs of symbols (bigrams) and find the most frequent ones in the corpus."
      ],
      "metadata": {
        "id": "_SE6DUegwjlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "QZ_cP8IObg31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #ADD YOUR CODE AS NECESSARY\n",
        "\n",
        "\n",
        "def bpe_train(corpus,targetbpe):\n",
        "    tokenized = []\n",
        "    with open(corpus) as f:  #read corpus, split in sents, put underscore after every word\n",
        "        sents = []\n",
        "        for line in f:\n",
        "          sent = line.split()\n",
        "          sents.append(sent)\n",
        "        flat_list = [f'{item}_' for sublist in sents for item in sublist]\n",
        "        tokenized = [char for word in flat_list for char in word] #tokenize the corpus\n",
        "    vocab = []  \n",
        "    i = 0  \n",
        "    while i < targetbpe:                                          #repeat untill vocab has targetbpe amount of words\n",
        "      data = Counter(bigrams(tokenized))\n",
        "      most_common = data.most_common(1)                           #take most common pair\n",
        "      vocab.append(most_common[0][0][0] + most_common[0][0][1])   #append to vocab\n",
        "      tokenized = replace_bp(tokenized, most_common[0][0][0] + most_common[0][0][1]) #merge the pair in the text\n",
        "      i += 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "8s7Z4QYH0MNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train subword vocabularies of 400 items on small Dutch and English corpora."
      ],
      "metadata": {
        "id": "oPeB0Xi-w7t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nl_bpes=bpe_train(\"data/en_nl/tiny_en-nl/train.trg\",400) #takes about 2 minutes"
      ],
      "metadata": {
        "id": "M0JFuprx8Pvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_bpes=bpe_train(\"data/en_nl/tiny_en-nl/train.src\",400)"
      ],
      "metadata": {
        "id": "A2HbGQUeeAPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the resulting list of subword units?"
      ],
      "metadata": {
        "id": "TAvmFl-dxCc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nl_bpes"
      ],
      "metadata": {
        "id": "c9tT3SW58xDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a27f89-120d-4a0a-fff5-0152d88db7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['en',\n",
              " 'e_',\n",
              " 't_',\n",
              " 'en_',\n",
              " 'er',\n",
              " '._',\n",
              " 'an',\n",
              " 'ij',\n",
              " 'in',\n",
              " 'et_',\n",
              " 'de_',\n",
              " 's_',\n",
              " 'el',\n",
              " 'aa',\n",
              " 'an_',\n",
              " ',_',\n",
              " 'or',\n",
              " 'k_',\n",
              " 'ch',\n",
              " 'st',\n",
              " 'ge',\n",
              " 'on',\n",
              " 'g_',\n",
              " 'er_',\n",
              " 'van_',\n",
              " 'aar',\n",
              " 'd_',\n",
              " 'at_',\n",
              " 'al',\n",
              " 'ie',\n",
              " 'oor',\n",
              " 'en._',\n",
              " 'he',\n",
              " 'het_',\n",
              " 'om',\n",
              " 'ver',\n",
              " 'be',\n",
              " 'een_',\n",
              " 'di',\n",
              " '?_',\n",
              " 'n_',\n",
              " 'op',\n",
              " 'oe',\n",
              " 'je_',\n",
              " 'is_',\n",
              " 'ij_',\n",
              " 'aar_',\n",
              " 're',\n",
              " 'dat_',\n",
              " 'at',\n",
              " 'in_',\n",
              " 'ui',\n",
              " 'te_',\n",
              " 'voor',\n",
              " 'le',\n",
              " 'sch',\n",
              " 'Ik_',\n",
              " 'ni',\n",
              " 'ar',\n",
              " 'ou',\n",
              " 'ing_',\n",
              " '-_',\n",
              " 'ie_',\n",
              " 'oo',\n",
              " 'ik_',\n",
              " 'el_',\n",
              " 't._',\n",
              " 'de',\n",
              " 'den_',\n",
              " 'niet_',\n",
              " 'we',\n",
              " 'der',\n",
              " 'ijn_',\n",
              " 'is',\n",
              " 'ur',\n",
              " 'il',\n",
              " 'ing',\n",
              " 'op_',\n",
              " 'me',\n",
              " 'ten_',\n",
              " 'ze_',\n",
              " 'voor_',\n",
              " 'heb',\n",
              " 'f_',\n",
              " 'ro',\n",
              " 'gen',\n",
              " 'om_',\n",
              " 'aan',\n",
              " 'met_',\n",
              " 'te',\n",
              " 'ra',\n",
              " 'ol',\n",
              " 'd._',\n",
              " 'die_',\n",
              " 'zi',\n",
              " 'cht',\n",
              " 'aan_',\n",
              " 'to',\n",
              " 'en,_',\n",
              " '..',\n",
              " 'ijk',\n",
              " 'm_',\n",
              " 'a_',\n",
              " 'gel',\n",
              " 'wa',\n",
              " 'wor',\n",
              " 'do',\n",
              " 'gen_',\n",
              " '..._',\n",
              " 'zijn_',\n",
              " 'ee',\n",
              " 'ul',\n",
              " 'nie',\n",
              " 'ben_',\n",
              " 'sp',\n",
              " 'al_',\n",
              " 'ter',\n",
              " 'un',\n",
              " 'ig',\n",
              " 'ze',\n",
              " 'id',\n",
              " 'Het_',\n",
              " 'am',\n",
              " 'u_',\n",
              " 's._',\n",
              " 'ijk_',\n",
              " 'wer',\n",
              " 'ik',\n",
              " 'De_',\n",
              " 'als_',\n",
              " 'ef',\n",
              " 'we_',\n",
              " 'it',\n",
              " 'den',\n",
              " 'gev',\n",
              " 'cht_',\n",
              " 'mo',\n",
              " 'aat_',\n",
              " 'ak',\n",
              " 'je',\n",
              " 'no',\n",
              " 'kom',\n",
              " 'st_',\n",
              " 'zo',\n",
              " 'ap',\n",
              " 'me_',\n",
              " 'sl',\n",
              " 'em',\n",
              " 'aal',\n",
              " 'der_',\n",
              " 'del',\n",
              " 'ss',\n",
              " 'ri',\n",
              " 'ijn',\n",
              " 'wil',\n",
              " 'waar',\n",
              " 'rij',\n",
              " 'uit',\n",
              " 'ov',\n",
              " 'eer',\n",
              " 'ou_',\n",
              " 'ken',\n",
              " 'bl',\n",
              " 'of_',\n",
              " 'Dat_',\n",
              " 'ct',\n",
              " 'heb_',\n",
              " 'all',\n",
              " 'moe',\n",
              " ')_',\n",
              " 'wat_',\n",
              " 'pro',\n",
              " 'hi',\n",
              " '!_',\n",
              " 'ingen_',\n",
              " 'oor_',\n",
              " 'Je_',\n",
              " 'men',\n",
              " 'heef',\n",
              " 'it_',\n",
              " 've',\n",
              " 'maar_',\n",
              " 'Al',\n",
              " 'naar_',\n",
              " 'ken_',\n",
              " 'ouw',\n",
              " 'id_',\n",
              " 'ell',\n",
              " 'lan',\n",
              " 'hij_',\n",
              " 'gr',\n",
              " 'tre',\n",
              " 'br',\n",
              " 'nen_',\n",
              " 'ont',\n",
              " 'kan_',\n",
              " 'af',\n",
              " 'over',\n",
              " 'heeft_',\n",
              " 'een',\n",
              " 'was_',\n",
              " 'dit_',\n",
              " 'se_',\n",
              " 't,_',\n",
              " 'We_',\n",
              " 'Wat_',\n",
              " 'bij_',\n",
              " 'g._',\n",
              " ':_',\n",
              " 'dt_',\n",
              " 'worden_',\n",
              " 'd,_',\n",
              " 'en?_',\n",
              " 'over_',\n",
              " 'vol',\n",
              " 'man',\n",
              " 'hebben_',\n",
              " 'moet_',\n",
              " '00',\n",
              " 'goe',\n",
              " 'du',\n",
              " 'ts_',\n",
              " 'kun',\n",
              " 'ha',\n",
              " 'mijn_',\n",
              " 'den._',\n",
              " 'da',\n",
              " 'ste_',\n",
              " 'ven_',\n",
              " 'En_',\n",
              " 'deze_',\n",
              " 'uit_',\n",
              " 'onder',\n",
              " 'er._',\n",
              " 'tij',\n",
              " 'ull',\n",
              " 'door_',\n",
              " 'mis',\n",
              " 'on_',\n",
              " 'geen_',\n",
              " 'dan_',\n",
              " 'toe',\n",
              " 'dr',\n",
              " 'ende_',\n",
              " 'isch',\n",
              " 'han',\n",
              " 'oe_',\n",
              " 'bel',\n",
              " 'Hij_',\n",
              " 'nog_',\n",
              " 'p_',\n",
              " \"'n_\",\n",
              " 'kt_',\n",
              " 'Ver',\n",
              " 'vr',\n",
              " 'ander',\n",
              " 'Ze_',\n",
              " 'la',\n",
              " 'aal_',\n",
              " 'ic',\n",
              " 'uur',\n",
              " 'zel',\n",
              " 'wij',\n",
              " 'aten_',\n",
              " 'ent_',\n",
              " 'ook_',\n",
              " 'lo',\n",
              " 'ger',\n",
              " 'tot_',\n",
              " 'zijn',\n",
              " 'art',\n",
              " 'vo',\n",
              " 'k._',\n",
              " 'haar_',\n",
              " 'dig',\n",
              " 'wil_',\n",
              " 'ent',\n",
              " 'ch_',\n",
              " 'hem_',\n",
              " 'van',\n",
              " 'wel',\n",
              " 'atie',\n",
              " 'wordt_',\n",
              " 'ff',\n",
              " 'ster',\n",
              " 'zal_',\n",
              " 'r_',\n",
              " 'con',\n",
              " 'niet._',\n",
              " 'hel',\n",
              " 'ma',\n",
              " 'i_',\n",
              " 'maa',\n",
              " 'va',\n",
              " 'li',\n",
              " 'et',\n",
              " 'gen._',\n",
              " 'moeten_',\n",
              " 'ijke_',\n",
              " 'ale_',\n",
              " 'heid_',\n",
              " 'weet_',\n",
              " 'aan._',\n",
              " 'Nee',\n",
              " 'au',\n",
              " 'eer_',\n",
              " 'onder_',\n",
              " 'werk',\n",
              " 'hou',\n",
              " 'ci',\n",
              " 'ig_',\n",
              " 'hu',\n",
              " 'zou_',\n",
              " 'Ja',\n",
              " 's,_',\n",
              " 'schap',\n",
              " 'atie_',\n",
              " 'hier_',\n",
              " 'per',\n",
              " '0_',\n",
              " 'miss',\n",
              " 'Eur',\n",
              " 'zo_',\n",
              " 'kunnen_',\n",
              " 'y_',\n",
              " 'Maar_',\n",
              " 'wel_',\n",
              " 'orm',\n",
              " 'laa',\n",
              " 'beg',\n",
              " 'o_',\n",
              " 'pa',\n",
              " 'ige_',\n",
              " 'ru',\n",
              " 'stell',\n",
              " 'ke_',\n",
              " 'ant',\n",
              " 'Europ',\n",
              " 'ati',\n",
              " 'Com',\n",
              " 'Een_',\n",
              " 'ons_',\n",
              " 'geb',\n",
              " 'ker',\n",
              " 'ikel_',\n",
              " 'Als_',\n",
              " 'alle',\n",
              " 'uw',\n",
              " 'ische_',\n",
              " 'meer_',\n",
              " 'jij_',\n",
              " '200',\n",
              " 'wee',\n",
              " 'ter_',\n",
              " 'Ge',\n",
              " 'mak',\n",
              " 'bij',\n",
              " 'kel',\n",
              " 'dig_',\n",
              " 'min',\n",
              " 'iets_',\n",
              " 'ken._',\n",
              " 'had_',\n",
              " 'stel',\n",
              " 'acht',\n",
              " 'Dit_',\n",
              " 'ber',\n",
              " 'dan',\n",
              " 'ies_',\n",
              " 'tijd_',\n",
              " 'kl',\n",
              " 'ullie_',\n",
              " 'gaat_',\n",
              " 'tr',\n",
              " 'zoe',\n",
              " 'dra',\n",
              " '19',\n",
              " 'je._',\n",
              " 'us_',\n",
              " 'ite',\n",
              " 'ouw_',\n",
              " 'mij_',\n",
              " 'nem',\n",
              " 'war',\n",
              " 'tel',\n",
              " 'ei',\n",
              " 't?_',\n",
              " 'Commiss',\n",
              " 'zeg',\n",
              " 'ko',\n",
              " 'zijn._',\n",
              " 'vin',\n",
              " '1_',\n",
              " 'ste',\n",
              " 'ten._',\n",
              " 'pen_',\n",
              " 'schi',\n",
              " 'Er_',\n",
              " 'brui',\n",
              " 'ge_']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_bpes"
      ],
      "metadata": {
        "id": "t4Ea2TYtfA7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3808830a-bdaa-4045-d822-8c733db99fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['e_',\n",
              " 'th',\n",
              " '._',\n",
              " 't_',\n",
              " 's_',\n",
              " 'in',\n",
              " 'd_',\n",
              " 'er',\n",
              " 'ou',\n",
              " 'on',\n",
              " 'an',\n",
              " ',_',\n",
              " 'the_',\n",
              " 'o_',\n",
              " 'y_',\n",
              " 'en',\n",
              " 'or',\n",
              " 'ti',\n",
              " 're',\n",
              " 'al',\n",
              " 'ing',\n",
              " 'ar',\n",
              " 'f_',\n",
              " 'to_',\n",
              " 'you',\n",
              " 'of_',\n",
              " 'a_',\n",
              " 'ing_',\n",
              " 'ha',\n",
              " 'on_',\n",
              " 'om',\n",
              " 'ed_',\n",
              " '?_',\n",
              " 'l_',\n",
              " 'is_',\n",
              " 'I_',\n",
              " 'at',\n",
              " 'es',\n",
              " 'it',\n",
              " 'er_',\n",
              " 'and_',\n",
              " 'is',\n",
              " 'in_',\n",
              " 'el',\n",
              " 'ow',\n",
              " 'e._',\n",
              " 'you_',\n",
              " 'ic',\n",
              " 've_',\n",
              " \"'s_\",\n",
              " '-_',\n",
              " 'or_',\n",
              " 'ro',\n",
              " 'ac',\n",
              " 'st',\n",
              " 'at_',\n",
              " 'ec',\n",
              " 'es_',\n",
              " 'us',\n",
              " 'an_',\n",
              " 'Th',\n",
              " 'as',\n",
              " 'ati',\n",
              " 'm_',\n",
              " 'il',\n",
              " 'k_',\n",
              " 'r_',\n",
              " 'en_',\n",
              " 'le',\n",
              " 'ig',\n",
              " 'al_',\n",
              " 'ot_',\n",
              " 'ol',\n",
              " 'as_',\n",
              " 'for_',\n",
              " \"'t_\",\n",
              " 'that_',\n",
              " 'wh',\n",
              " 'id',\n",
              " 'ur',\n",
              " 'h_',\n",
              " 'th_',\n",
              " 'it_',\n",
              " '!_',\n",
              " 'be_',\n",
              " 'sh',\n",
              " 'ab',\n",
              " 'ent',\n",
              " 'You',\n",
              " 'em',\n",
              " 'oo',\n",
              " 'le_',\n",
              " 'un',\n",
              " 'y._',\n",
              " 'wi',\n",
              " 'ad',\n",
              " 'am',\n",
              " 'ed',\n",
              " 'ly_',\n",
              " 'li',\n",
              " \"I'\",\n",
              " 'pro',\n",
              " 'et',\n",
              " 's._',\n",
              " 'ow_',\n",
              " 'os',\n",
              " 'e,_',\n",
              " 'tr',\n",
              " 'ev',\n",
              " 'igh',\n",
              " 'ul',\n",
              " 'op',\n",
              " '..',\n",
              " 'ere_',\n",
              " 'with_',\n",
              " 'hat_',\n",
              " 'con',\n",
              " 'uc',\n",
              " 'ag',\n",
              " 'ent_',\n",
              " 'ke_',\n",
              " 'im',\n",
              " 'have_',\n",
              " 'oul',\n",
              " 'are_',\n",
              " 'You_',\n",
              " 'ch',\n",
              " 'et_',\n",
              " 'not_',\n",
              " 'all_',\n",
              " 'ay',\n",
              " '..._',\n",
              " 'the',\n",
              " 're_',\n",
              " 'out_',\n",
              " 'ation_',\n",
              " 'ex',\n",
              " 'n_',\n",
              " 'com',\n",
              " 'ap',\n",
              " 'di',\n",
              " 'up',\n",
              " 'this_',\n",
              " 'be',\n",
              " 'res',\n",
              " 'kn',\n",
              " 'was_',\n",
              " 'ir',\n",
              " 'The_',\n",
              " 'me_',\n",
              " 'ay_',\n",
              " 'for',\n",
              " \"t's_\",\n",
              " 'ould_',\n",
              " 'ut_',\n",
              " 't._',\n",
              " 'oun',\n",
              " 'ust_',\n",
              " 'fr',\n",
              " 'your_',\n",
              " 'll_',\n",
              " 'si',\n",
              " \"I'm_\",\n",
              " 'y,_',\n",
              " 'pl',\n",
              " 'my_',\n",
              " 'ter',\n",
              " 'tic',\n",
              " \"'re_\",\n",
              " \"on't_\",\n",
              " 'we_',\n",
              " ')_',\n",
              " 'ta',\n",
              " 'ea',\n",
              " 'ear',\n",
              " 'An',\n",
              " 'ation',\n",
              " 'ell',\n",
              " 'ce_',\n",
              " 'h,_',\n",
              " 's,_',\n",
              " 'one_',\n",
              " 'ver',\n",
              " 'do',\n",
              " 'me',\n",
              " 'Com',\n",
              " 'ne',\n",
              " 'se',\n",
              " 'ther',\n",
              " 'tion_',\n",
              " 'her',\n",
              " \"e's_\",\n",
              " 'su',\n",
              " 'and',\n",
              " 'om_',\n",
              " 'go',\n",
              " 'tion',\n",
              " 'ff',\n",
              " 'by_',\n",
              " 'wor',\n",
              " 'pe',\n",
              " 'qu',\n",
              " 'ther_',\n",
              " 'ri',\n",
              " 'ant_',\n",
              " 'do_',\n",
              " 'par',\n",
              " 'he_',\n",
              " 'all',\n",
              " 'ts_',\n",
              " \"n't_\",\n",
              " 'ma',\n",
              " 'to',\n",
              " 'por',\n",
              " 'that',\n",
              " 'ill_',\n",
              " 'can_',\n",
              " 'd._',\n",
              " 'es._',\n",
              " \"don't_\",\n",
              " 'what_',\n",
              " 'ep',\n",
              " 'um',\n",
              " 'get_',\n",
              " '00',\n",
              " 'Wh',\n",
              " 'som',\n",
              " 'righ',\n",
              " 'And_',\n",
              " 'we',\n",
              " 'ci',\n",
              " 'just_',\n",
              " 'king_',\n",
              " 'know_',\n",
              " 'lo',\n",
              " 'so_',\n",
              " 'ut',\n",
              " 'from_',\n",
              " 'ate_',\n",
              " ':_',\n",
              " 'ing._',\n",
              " 'bo',\n",
              " 'We_',\n",
              " 'Comm',\n",
              " 'no',\n",
              " 'per',\n",
              " 'w_',\n",
              " \"It's_\",\n",
              " 'ber_',\n",
              " 'oug',\n",
              " 'ter_',\n",
              " 'me._',\n",
              " 'gr',\n",
              " 'No',\n",
              " 'will_',\n",
              " 'ity_',\n",
              " 'bl',\n",
              " 'about_',\n",
              " 'cl',\n",
              " 'like_',\n",
              " 'it._',\n",
              " 'id_',\n",
              " 'What_',\n",
              " 'es,_',\n",
              " 'pos',\n",
              " 't,_',\n",
              " 'gi',\n",
              " 'rec',\n",
              " 'e?_',\n",
              " 'ell_',\n",
              " 'ally_',\n",
              " 'thing_',\n",
              " 'mor',\n",
              " 'but_',\n",
              " 'ic_',\n",
              " 'sp',\n",
              " 'thin',\n",
              " 'sa',\n",
              " 'end',\n",
              " 'our_',\n",
              " 'ted_',\n",
              " 'ed._',\n",
              " 'use_',\n",
              " 'our',\n",
              " 'fin',\n",
              " 'de',\n",
              " 'they_',\n",
              " 'ain',\n",
              " 'got_',\n",
              " 'St',\n",
              " 'up_',\n",
              " 'This_',\n",
              " 'man',\n",
              " 'pr',\n",
              " 'ting_',\n",
              " 'hi',\n",
              " 'ell,_',\n",
              " 't?_',\n",
              " 'Ar',\n",
              " \"hat's_\",\n",
              " 'if_',\n",
              " 'you._',\n",
              " 'ud',\n",
              " 'ure_',\n",
              " 'ir_',\n",
              " 'ich_',\n",
              " 'k._',\n",
              " 'mo',\n",
              " 'want_',\n",
              " 'min',\n",
              " 'It_',\n",
              " 'been_',\n",
              " 'co',\n",
              " 'und',\n",
              " 'ever',\n",
              " 'gu',\n",
              " 'own_',\n",
              " 'ord',\n",
              " 'which_',\n",
              " 'ong',\n",
              " 'av',\n",
              " 'duc',\n",
              " 'ay._',\n",
              " 'ember_',\n",
              " 'Oh,_',\n",
              " 'ood_',\n",
              " 'str',\n",
              " 'ang',\n",
              " 'ents_',\n",
              " 'has_',\n",
              " 'would_',\n",
              " 'na_',\n",
              " 'can',\n",
              " 'think_',\n",
              " 'p_',\n",
              " 'Well,_',\n",
              " 'bac',\n",
              " 'ure',\n",
              " 'uro',\n",
              " 'dis',\n",
              " 'are',\n",
              " 'He_',\n",
              " 'were_',\n",
              " 'eg',\n",
              " 'ever_',\n",
              " 'vi',\n",
              " 'on._',\n",
              " 'po',\n",
              " 'know',\n",
              " 'sion_',\n",
              " 'other_',\n",
              " 'sel',\n",
              " 'his_',\n",
              " 'ose_',\n",
              " 'la',\n",
              " 'acc',\n",
              " 'Artic',\n",
              " 'er._',\n",
              " 'itt',\n",
              " 'Euro',\n",
              " 'pre',\n",
              " 'ar_',\n",
              " 'gon',\n",
              " 'ess_',\n",
              " 'se_',\n",
              " 'more_',\n",
              " 'able_',\n",
              " 'fi',\n",
              " 'us_',\n",
              " 'had_',\n",
              " 'ment_',\n",
              " '0_',\n",
              " 'ers_',\n",
              " \"it's_\",\n",
              " 'ame_',\n",
              " 'should_',\n",
              " 'The',\n",
              " 'ated_',\n",
              " 'fe',\n",
              " 'did',\n",
              " 'ra',\n",
              " 'mar',\n",
              " 'any_',\n",
              " 'need_',\n",
              " '200',\n",
              " 'Article_',\n",
              " 'their_',\n",
              " 'tal',\n",
              " 'But_',\n",
              " 'pp',\n",
              " 'loo',\n",
              " 'its_',\n",
              " 'some',\n",
              " 'him',\n",
              " 'see_',\n",
              " \"'ll_\",\n",
              " 'ess',\n",
              " 'ance_',\n",
              " 'Yea',\n",
              " 'her_']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.4 (4 points) Now define function bpe_tokenize that breaks down an input word (a string) into subwords in a greedy left to right manner using a subword vocabulary. *Greedy* tokenization means that whenever there are multiple options to identify the next syte pair unit, the longest possible one is selected."
      ],
      "metadata": {
        "id": "a2d9Xa-KxOv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpe_tokenize(s,bpes):\n",
        "#YOUR CODE HERE  "
      ],
      "metadata": {
        "id": "64cAwvfv-OWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenize('university_',['_','u','n','i','v','e','r','s','t','y','univers'])"
      ],
      "metadata": {
        "id": "mXsNYo9QV3RP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fe1d2d-0d94-40b7-8ce0-ca6dc3a4564c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['u', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', '_']"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenize('nederlandse_',nl_bpes)"
      ],
      "metadata": {
        "id": "-PXHTBN1_34C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b85d02-bf92-4bbc-af6e-f1b27911476f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['n', 'e', 'de', 'r', 'lan', 'd', 'se_']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenize('whenever_',en_bpes)"
      ],
      "metadata": {
        "id": "-BWfWndqLsQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6693b6c-8e95-4393-9318-01a1bb08820b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wh', 'en', 'ev', 'er', '_']"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    }
  ]
}