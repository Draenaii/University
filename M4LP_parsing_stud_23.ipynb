{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Draenaii/University/blob/main/M4LP_parsing_stud_23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1 ([M4LP](https://osiris.uu.nl/osiris_student_uuprd/OnderwijsCatalogusSelect.do?selectie=cursus&collegejaar=2021&cursus=KI3V21001))\n",
        "\n",
        "The assignment covers dependency and constituency parsing.  \n",
        "<font color=\"red\">**The rules to follow**:</font>  \n",
        "* Don't delete any initially provided cells, either text or code cells (but you should delete certain lines in the cells, continue reading).\n",
        "* Don't delete the exercise code header `#...# EXERCISE n #..#` lines and the `# TEST` lines. \n",
        "* Don't change the names of provided functions and variables. \n",
        "* If you skip an exercise, then delete all lines in the cell following the header `#...# EXERCISE n #..#` but leave its backup part--the line with `IFSKIPPED` and its following lines.\n",
        "* If you solve an exercise, then delete its corresponding backup part starting with `IFSKIPPED` and the following lines. \n",
        "* Use global vars throughout your code and change only those globals vars that are explicitly instructed. \n",
        "* For `#TEST` cells, if its output is coming from your code, then leave it; otherwise clear the output of the cell as it is uninformative and clutters the ipynb. \n",
        "* For Text cells, you are expected to insert your input only in the cells that come with a red section title. \n",
        "* Name the ipynb file with your group number before submitting, e.g., `01.ipynb` or `11.ipynb`.\n",
        "\n",
        "<font color=\"red\">You following these rules helps us to grade the submissions relatively efficiently. If these rules are violated, a submission will be subject to penalty points.</font>  \n",
        "\n",
        "<font color=\"red\">**IMPORTANT**</font>: you are strongly encouraged to use Google Colab when solving the exercises. Setting the common environment prevents students and teachers from various headaches related to cross-platform variations, module/package versioning, and unpredicted behaviour of the code. In this way, we try that you spend as much time as possible on coding during the course rather than on installations. Moreover, colab notebooks are very practical for group collaboration as they come with version history and several persons can work on the same notebook (not simultaneously though).  \n",
        "You are still free to solve the exercises on your own machine but in the end, make sure that your solutions also work in the colab environment. \n",
        "\n",
        "by L.abzianidze@uu.nl"
      ],
      "metadata": {
        "id": "7gbltP-WdxjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\">Contributions</font>\n",
        "\n",
        "~~Delete this text and write instead of it your:~~\n",
        "* ~~group number (same as the file name, for sanity chack)~~\n",
        "* ~~a list of group members names (NOT student IDs)~~\n",
        "* ~~who contributed to which exercises (you don't need to be very detailed)~~ "
      ],
      "metadata": {
        "id": "nNT1WNEnlkBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "2RMnck5Qy-Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "Import spaCy and download its model. Install Stanza that comes with an interface for CoreNLP. Download CoreNLP. Install modules and prepare the environment for rendering syntactic trees of NLTK. Download a course-specific python package that contains useful tools.  \n",
        "\n",
        "Additionally, you might find the following predefined function(s) handy: [isinstance](https://www.programiz.com/python-programming/methods/built-in/isinstance), [list comprehension](https://www.programiz.com/python-programming/list-comprehension), [f-string](https://www.geeksforgeeks.org/formatted-string-literals-f-strings-python/)"
      ],
      "metadata": {
        "id": "SN0KF3t-eykf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "if spacy.__version__ != '3.5.2':\n",
        "    print(f\"spaCy v={spacy.__version__} but it should be 3.5.2\\nForce install 3.5.2 with the next cell\")"
      ],
      "metadata": {
        "id": "3aLaP307Rr0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3blhWf07xC1n"
      },
      "outputs": [],
      "source": [
        "# may require environment restart\n",
        "# !pip install spacy==3.5.2\n",
        "\n",
        "# if this cell errors with \"A UTF-8 locale is required. Got ANSI_X3.4-1968\"\n",
        "# uncomment and run the next two lines \n",
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "FcKCoPmGSrtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "if stanza.__version__ != '1.5.0':\n",
        "    print(f\"stanza v={stanza.__version__} but it should be 1.5.0\\nForce install 1.5.0 with the next cell\")"
      ],
      "metadata": {
        "id": "BB7bt08hTDMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# may require environment restart\n",
        "# !pip install stanza==1.5.0"
      ],
      "metadata": {
        "id": "as0rrm8hLInv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Download the Stanford CoreNLP package with Stanza's installation command\n",
        "# This'll take several minutes, depending on the network speed\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "# Import client module\n",
        "from stanza.server import CoreNLPClient\n",
        "# src: https://github.com/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb"
      ],
      "metadata": {
        "id": "jEcBRpafDJGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Needed to display NLTK's trees objects\n",
        "!pip install svgling"
      ],
      "metadata": {
        "id": "slfQk5ooBVog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assigntools package is a course specific collection of useful tools \n",
        "! rm -rf assigntools\n",
        "! git clone https://github.com/kovvalsky/assigntools.git"
      ],
      "metadata": {
        "id": "kPIBvjxg9aN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "ziLWG53kfGST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display\n",
        "from spacy import displacy\n",
        "import importlib"
      ],
      "metadata": {
        "id": "JDIzxmBYxQV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Course-specific package\n",
        "\n",
        "from assigntools.M4LP.A1 import read_pickle, write_pickle, download_extract_zip, flatten_list, display_doc_dep"
      ],
      "metadata": {
        "id": "glcaegh-yHNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "print(f\"spaCy version: {spacy.__version__}\")    # should be 3.5.2\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"NLTK version: {nltk.__version__}\")\n",
        "print(f\"stranza version: {stanza.__version__}\") # should be 1.5.0"
      ],
      "metadata": {
        "id": "P3JBRKtzxSRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3791cf38-929d-4214-af61-6eec0bfdaff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy version: 3.5.2\n",
            "Python version: 3.9.16 (main, Dec  7 2022, 01:11:51) \n",
            "[GCC 9.4.0]\n",
            "NLTK version: 3.8.1\n",
            "stranza version: 1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "OLrqZ_pK0Of4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read-only\n",
        "# URL of a file that will be used during the assignmnet \n",
        "SICK_TRIAL_URL =  \"http://alt.qcri.org/semeval2014/task1/data/uploads/sick_trial.zip\"\n",
        "files = download_extract_zip(SICK_TRIAL_URL)"
      ],
      "metadata": {
        "id": "BHUPuYpr0T7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User modules\n",
        "\n",
        "Import all modules here what you might need in addition to what is already imported."
      ],
      "metadata": {
        "id": "HcD8C5xA1isO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT ALL ADDED AND NECESSARY MODULES HERE (IF ANY)\n"
      ],
      "metadata": {
        "id": "pTz2S1M61vAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex1[2pt]: Extracting sentences\n",
        "\n",
        "Often when parsing a bunch of sentences, it is a good practice to parse each sentence only once and decrease the parsing time. The number of sentences in this exercises are not too much, so saved parsing time in the end will be ~2-3min, but sometimes in real applications such tricks can save hours. \n",
        "\n",
        "The file the function is supposed to read is tab-seperated-value file. You can use string operations or regex to read the sentences but the best practice is to use ready modules that provide file readers for common file formats.   "
      ],
      "metadata": {
        "id": "cxE9qimJxZMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 1 ##################################\n",
        "################################################################################\n",
        "\n",
        "def read_tsv_sents(file_path):\n",
        "    \"\"\" Takes the path of a tab-seperated-value file and reads sentences from it.\n",
        "        Return a list of sentences that is sorted (in ascending order) \n",
        "        and duplicate ones are filtered out.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "# IFSKIPPED:\n",
        "# sents = read_pickle('sents.pkl') "
      ],
      "metadata": {
        "id": "gfkKCVkj8cqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EX1\n",
        "sents = read_tsv_sents('SICK_trial.txt')\n",
        "assert sents[:3] == ['A baby is playing with a doll', 'A baby is playing with a toy', 'A baby tiger is playing with a ball']\n"
      ],
      "metadata": {
        "id": "UPPcskQqEbDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex2[2pt]:Parsing and tagging with spaCy\n",
        "\n",
        "Now it is time to parse sentences. We will use [spaCy](https://spacy.io/) for getting dependency trees of the sentences. In addition to the dependency parsing, spaCy pipeline also does part-of-speech tagging and lemmatization (with other stuff). In this exercise, we print spaCy's token annotations in a style of a table. The fancy table is due to good integration of pandas Data Frames into Jupyter notebooks.   \n",
        "\n",
        "For a quick intro to spaCy, have a look at the following section in the [spaCy tutorial](https://course.spacy.io/en/): sections 1 and 5 in [chapter 1](https://course.spacy.io/en/chapter1), and 4 in [chapter 2](https://course.spacy.io/en/chapter2).   \n",
        "Use attributes of spaCy's [Token objects](https://spacy.io/api/token).  \n",
        "[10min to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) is a good start to learn basics about pandas Data Frames.  \n",
        "After annotation, tokens come with two pos tags: fine-grained corresponds to [Penn Treebank pos tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) while coarse-grained to [Universal pos tags](https://universaldependencies.org/u/pos/).  "
      ],
      "metadata": {
        "id": "puBKCjT4xTsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing all sentences with spaCy's small model\n",
        "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "docs_sm = list(nlp_sm.pipe(sents))"
      ],
      "metadata": {
        "id": "LSinzLPdyMb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing all sentences with spaCy's medium model\n",
        "nlp_md = spacy.load(\"en_core_web_md\")\n",
        "docs_md = list(nlp_md.pipe(sents))"
      ],
      "metadata": {
        "id": "KIZ2bJkHyJBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 2 ##################################\n",
        "################################################################################\n",
        "\n",
        "def doc_df_anno(doc):\n",
        "    \"\"\" Structure spaCy annotations in a pandas DataFrame and return it.\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "nYXnBSvb4RZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "doc_df_anno(docs_md[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "33Fcm3hg5BUD",
        "outputId": "13b83708-74fe-4dd3-cada-793e4a75b695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0:A   1:baby     2:is 3:playing   4:with   5:a 6:doll\n",
              "lemma         a     baby       be      play     with     a   doll\n",
              "pos         DET     NOUN      AUX      VERB      ADP   DET   NOUN\n",
              "tag          DT       NN      VBZ       VBG       IN    DT     NN\n",
              "dep         det    nsubj      aux      ROOT     prep   det   pobj\n",
              "head.i        1        3        3         3        3     6      4\n",
              "head.text  baby  playing  playing   playing  playing  doll   with"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed3f70bf-d3fa-44a0-8687-76dcf5fa4da6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0:A</th>\n",
              "      <th>1:baby</th>\n",
              "      <th>2:is</th>\n",
              "      <th>3:playing</th>\n",
              "      <th>4:with</th>\n",
              "      <th>5:a</th>\n",
              "      <th>6:doll</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lemma</th>\n",
              "      <td>a</td>\n",
              "      <td>baby</td>\n",
              "      <td>be</td>\n",
              "      <td>play</td>\n",
              "      <td>with</td>\n",
              "      <td>a</td>\n",
              "      <td>doll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pos</th>\n",
              "      <td>DET</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>AUX</td>\n",
              "      <td>VERB</td>\n",
              "      <td>ADP</td>\n",
              "      <td>DET</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tag</th>\n",
              "      <td>DT</td>\n",
              "      <td>NN</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>VBG</td>\n",
              "      <td>IN</td>\n",
              "      <td>DT</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dep</th>\n",
              "      <td>det</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>aux</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>prep</td>\n",
              "      <td>det</td>\n",
              "      <td>pobj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>head.i</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>head.text</th>\n",
              "      <td>baby</td>\n",
              "      <td>playing</td>\n",
              "      <td>playing</td>\n",
              "      <td>playing</td>\n",
              "      <td>playing</td>\n",
              "      <td>doll</td>\n",
              "      <td>with</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed3f70bf-d3fa-44a0-8687-76dcf5fa4da6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ed3f70bf-d3fa-44a0-8687-76dcf5fa4da6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ed3f70bf-d3fa-44a0-8687-76dcf5fa4da6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EX2: There is a visually better way to display spaCy's dependency trees with the help of displaCy\n",
        "display_doc_dep(docs_md[0])\n",
        "# we can regulate space between tokens, but it might affect readability of labels\n",
        "display_doc_dep(docs_md[1], d=100)"
      ],
      "metadata": {
        "id": "TVYMC-Vm0MZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex3[4pt]: Projectivity\n",
        "\n",
        "Use spaCy's [Token attributes or methods](https://spacy.io/api/token) related to dependency annotations. This will make code much much simpler. "
      ],
      "metadata": {
        "id": "iypVScvA8gmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 3 ##################################\n",
        "################################################################################\n",
        "\n",
        "def is_projective(doc):\n",
        "    \"\"\" Checks a dependency tree on projectivity. Uses the definition \n",
        "        of projective arcs and checks all ars on projectivity.  \n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "JE8_SC8gRXxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "for i, d in enumerate(docs_sm):\n",
        "    if not is_projective(d):\n",
        "        print(f\"{i}: {d}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxHJEuxn3WSt",
        "outputId": "5605eb41-9655-412a-b36d-23b4e2b52bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "689: The girl has red hair and eyebrows, several piercings in a ear and a drawing on the back\n",
            "690: The girl has red hair and eyebrows, several piercings in a ear and a tattoo on the back\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "for i, d in enumerate(docs_md):\n",
        "    if not is_projective(d):\n",
        "        print(f\"{i}: {d}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTmUJ_a_AlG6",
        "outputId": "3718b25d-3dd2-44e6-b533-a599b0f6682b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52: A boy under an umbrella is being held by his father who is wearing a blue coat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EX3: Let's see how non-projective dependency trees look like\n",
        "# using compat=False to better display crossing arcs \n",
        "display_doc_dep(docs_md[52], compact=False, d=100)"
      ],
      "metadata": {
        "id": "DUxtF8GPzsSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex4[2pt]: Annotation-based filtering\n",
        "\n",
        "Annotations provides additional info that can be used to filter data. Let's build a function that filters spaCy docs based on occurence on certain tags or lemmas. Use [Penn Treebank pos tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) for filtering.  "
      ],
      "metadata": {
        "id": "HoihLeXzBCpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 4 ##################################\n",
        "################################################################################\n",
        "\n",
        "def filter_docs(docs, lemmas=[], tags=[], verbose=False):\n",
        "    \"\"\" Filters out docs that contain no single token with a tag (i.e., .tag_)\n",
        "        OR a lemma (i.e., .lemma_) in the provided lemmas and tags lists.\n",
        "        Return the filtered docs (i.e., those that contain such a token).\n",
        "        verbose flag determines whether to print a message about the number of\n",
        "        returned docs.\n",
        "        Note that the returned docs needs to contain lemmas OR tags: \n",
        "        parameters are interpreted as disjunction \n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "NHgFV4w-O7bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EX4\n",
        "_ = filter_docs(docs_sm, tags=[\"NNS\"], lemmas=[\"dog\", \"cat\"], verbose=True)\n",
        "_ = filter_docs(docs_sm, tags=[\"VBD\"], lemmas=[\"she\"], verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcpQaH76O7eR",
        "outputId": "76527ed1-72f6-45bf-80d7-59b9cc4cbc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "385 docs contain tags (['NNS']) or lemmas (['dog', 'cat'])\n",
            "6 docs contain tags (['VBD']) or lemmas (['she'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing with CoreNLP\n",
        "\n",
        "CoreNLP will be used through [Stanza CoreNLP interface](https://github.com/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb). CoreNLP provides both constituency and dependency trees. For English, it is possible to directly get dependency trees with a dependency parser or indirectly obtain them by converting the constituency trees into dependecy trees. "
      ],
      "metadata": {
        "id": "HM1sj-8BpFtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting dependency trees from a dependency parser\n",
        "# takes <1min\n",
        "# https://stanfordnlp.github.io/CoreNLP/depparse.html\n",
        "with CoreNLPClient(annotators='tokenize,pos,depparse', \n",
        "                   memory='4G', endpoint='http://localhost:9021', be_quiet=True,\n",
        "                   output_format='json') as client:\n",
        "    core_dep_parses = [ client.annotate(s)['sentences'][0] for s in sents ]"
      ],
      "metadata": {
        "id": "MtH9IG39p4jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting dependency trees from a constituency parser\n",
        "# takes <2min\n",
        "# https://stanfordnlp.github.io/CoreNLP/parse.html\n",
        "with CoreNLPClient(annotators='tokenize,pos,parse', \n",
        "                   memory='4G', endpoint='http://localhost:9021', be_quiet=True,\n",
        "                   output_format='json') as client:\n",
        "    core_con_parses = [ client.annotate(s)['sentences'][0] for s in sents ]"
      ],
      "metadata": {
        "id": "ttK7fSg4LFx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Drawing CoreNLP constituency trees with NLTK's Tree object\n",
        "Tree.fromstring(core_con_parses[0]['parse'])"
      ],
      "metadata": {
        "id": "slfJfHDKoi98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex5:[3pt] From CoreNLP to Doc\n",
        "\n",
        "CoreNLP dependencies, e.g., `core_dep_parses[0]['basicDependencies']` are a list of dictionaries each corresponding to a token. It would be handy if the dependencies are formatted as spaCy's [Doc object](https://spacy.io/api/doc), which allows to display dependency trees (or check projectivity). Read how [Doc](https://spacy.io/api/doc) can be initialized. You should find `core_dep_parses[0]['tokens']` useful for getting values of `spaces` and `tags` arguments.   \n",
        "\n"
      ],
      "metadata": {
        "id": "k3ECTHM0Lsla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 5 ##################################\n",
        "################################################################################\n",
        "\n",
        "def coreNLP2Doc(parse, nlp):\n",
        "    \"\"\" Uses info from parse['basicDependencies'] and parse['tokens'] \n",
        "        to initialize and return a Doc object.  \n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "OZeqe9xx_RwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "# coreNLP direct dependencies\n",
        "d_dep = coreNLP2Doc(core_dep_parses[0], nlp_md)\n",
        "display_doc_dep(d_dep, d=95, compact=False)\n",
        "# coreNLP indirect dependencies\n",
        "d_con = coreNLP2Doc(core_con_parses[0], nlp_md)\n",
        "display_doc_dep(d_con, d=95, compact=False)"
      ],
      "metadata": {
        "id": "W-KS6HEABsG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatting CoreNLP dependecies as Doc objects\n",
        "docs_dep = [ coreNLP2Doc(p, nlp_md) for p in core_dep_parses ]\n",
        "docs_con = [ coreNLP2Doc(p, nlp_md) for p in core_con_parses ]\n",
        "\n",
        "\n",
        "# IFSKIPPED EX5:\n",
        "# docs_dep = read_pickle('docs_dep.pkl') \n",
        "# docs_con = read_pickle('docs_con.pkl')"
      ],
      "metadata": {
        "id": "jm9OJ3MXqisf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EX5: checking projectivity of dependencies induced from constituencies\n",
        "for i, d in enumerate(docs_dep):\n",
        "    if not is_projective(d):\n",
        "        print(f\"{i}: {d}\")"
      ],
      "metadata": {
        "id": "fHRk8RrWQDZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex6[4pt]: PP attachment\n",
        "\n",
        "Correctly attaching prepositional phrases is a hard task in syntactic (and semantic) parsing. Now we will compare to what extent dependencies from small and medium models of spaCy differ from each other for the same sentences. We will compare how dependency trees attach prepositions to the heads (for simplicity we will ignore the dependency labels)."
      ],
      "metadata": {
        "id": "A7yoam3fSF5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 6 ##################################\n",
        "################################################################################\n",
        "\n",
        "def compare_pp_attachments(docs1, docs2, verbose=False):\n",
        "    \"\"\" Takes two lists of dependencies (of the same length) and searches the indices of docs \n",
        "        for which the corresponding dependencies differ in attaching the prepositions \n",
        "        (i.e., heads of the preposition tokens are different).\n",
        "        Prepositions are detecting with their tags. When docs1 and docs2 tag the same preposition\n",
        "        differently, such cases are ignored in pp-attachment comparison. \n",
        "        Return a list of indices of differing docs (indexing starts from 0)\n",
        "        When verbose is on, it reports and displays the corresponding contrasting dependencies\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "OMeBek9hSFXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "indices = compare_pp_attachments(docs_sm, docs_md)[:2]\n",
        "print(f\"Doc with the following indices are having different pp-attachments:\\n{indices}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r12sbn2fVWAW",
        "outputId": "6b71348d-2cfa-4760-8736-ad7afd39b815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc with the following indices are having different pp-attachments:\n",
            "[10, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EX6\n",
        "compare_pp_attachments(docs_sm[:15], docs_md[:15], verbose=True)\n",
        "# displayed images might be too wide, so use left & right arrow to scroll them horizontally  "
      ],
      "metadata": {
        "id": "rH9IZaYm9bHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\">Ex7[4pt]: Contrasting PP-attachments</font>\n",
        "\n",
        "Replace this text (only leave the header) with your answer to the following task.  \n",
        "Select **two sentences with different pp attachments** whose index is >100. Provide your argument why one of the pp attachments is better than another.  \n",
        "Pick those sentences for which you are able to provide a sound argument. \n",
        "In your answer, for each example, make it clear which documents are discussed, which preposition attachments are contrasted, which one is better and why.  \n",
        "Also display the corresponding dependency trees (in total 4) below this cell using `display_doc_dep` function."
      ],
      "metadata": {
        "id": "LJTuXVo_57V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## EXERCISE 7 ##################################\n",
        "# INSERT YOUR CODE TO DISPLAY CONTRASTING DEPENDENCIES "
      ],
      "metadata": {
        "id": "s2ECVSB--hUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex8[5pt]: From dependency to list constituency\n",
        "\n",
        "Syntactic trees are usually recursive structures, and so is dependency trees. This means that a dependency tree can be seen as a root node having arcs to the roots of sub-dependency trees. Processing recursive strcutures often requires recursive functions. Usually recursive functions are concise but writing them might not be easy if one is not familiar with recursive procedures. So, we would like you to practice writing recursive functions to process recursive structures. While doing this, we will still be using syntactic trees.\n",
        "\n",
        "Write a function that converts spaCy's dependencies into lists of lists (with unbounded depth) of strings, where strings are token texts. We will call this structure list constituency (in short \"lc\") because list brackets naturally express constituency info. Use might find attributes/methods/properties of spaCy's [Token objects](https://spacy.io/api/token) very useful.\n",
        "\n",
        "Non-projective dependencies are not convertible into a list constituency (think yourself why). For the exercise, it doesn't matter what happens to non-projective dependencies (and anyway they are few). Both are ok whether `dep2lc` errors on non-projective dependencies or provides a wrong list constituency.\n",
        "\n"
      ],
      "metadata": {
        "id": "fxBhr3geVWft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 8 ##################################\n",
        "################################################################################\n",
        "\n",
        "def dep2lc(doc):\n",
        "    \"\"\" Takes spaCy's doc that comes with dependency annotation and converts\n",
        "        it into constituency tree where constituents are represented as lists.\n",
        "        Nested constituencies are automatically expressed as nested lists.\n",
        "        This is a recursive function that should call itself in its body.\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "6sHsqH_tzo4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "print(dep2lc(docs_sm[37]))\n",
        "print(dep2lc(docs_md[37]))\n",
        "print(dep2lc(docs_dep[37]))\n",
        "print(dep2lc(docs_con[37]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TRuvZt8ECWl",
        "outputId": "e0ed4d98-ee36-4408-a9e2-40af06cc247d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['A', 'boat'], 'is', 'sailing', 'peacefully', ['over', ['the', 'water']]]\n",
            "[['A', 'boat'], 'is', 'sailing', 'peacefully', ['over', ['the', 'water']]]\n",
            "[['A', 'boat'], 'is', 'sailing', 'peacefully', ['over', 'the', 'water']]\n",
            "[['A', 'boat'], 'is', 'sailing', 'peacefully', ['over', 'the', 'water']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EX8: note that list constituency is also a recursive struicture.\n",
        "# Here is an example how a recursive function that flatterns it.\n",
        "# You can find the body of the function in its git repo (see how flatten_list is imported).\n",
        "flatten_list(dep2lc(docs_sm[37]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuZ6v_nsYPYA",
        "outputId": "eed0dd8e-1abf-40fe-beba-8e7ecb824ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'boat', 'is', 'sailing', 'peacefully', 'over', 'the', 'water']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex9[4pt]: From phrase structures to list constituency\n",
        "\n",
        "Now we are converting [NLTK's Tree object](https://www.nltk.org/howto/tree.html), which encodes phrase structures trees, into a list constituency. As in Ex9, it is easiest to process a recursive structure with a recursive function. You are provided with a ready function `corenlp2lc` that takes care of converting CoreNLP parses into NLTK's Trees and it employs `tree2lc` that you need to implement."
      ],
      "metadata": {
        "id": "CAoaH-jMGCVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 9 #################################\n",
        "################################################################################\n",
        "\n",
        "def tree2lc(tree):\n",
        "    \"\"\" Takes NLTK's Tree object which encodes a phrase structure tree and\n",
        "        converts it into a list representation of constituency (as in Ex9).\n",
        "        Nested constituencies are automatically expressed as nested lists.\n",
        "        This is a recursive function that should call itself in its body.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def corenlp2lc(analysis):\n",
        "    \"\"\" A wrapper function for converting phrase structures to list constituency.\n",
        "        The wrapper takes care of converting CoreNLP's \n",
        "        phrase structure trees into NLTK's Tree objects\n",
        "    \"\"\"\n",
        "    tree = Tree.fromstring(analysis['parse'])\n",
        "    return tree2lc(tree)"
      ],
      "metadata": {
        "id": "KEJqZWGeGcoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EX9\n",
        "corenlp2lc(core_con_parses[37])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF7ONhP7Hoaa",
        "outputId": "bea11508-def0-4ab5-dfd7-34459b29d777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['A', 'boat'],\n",
              "  ['is', ['sailing', ['peacefully'], ['over', ['the', 'water']]]]]]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}